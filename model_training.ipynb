{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413235e6",
   "metadata": {},
   "source": [
    "# Prediction Project - Acceptance Rate (%) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306dba6f",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655800f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, accuracy_score, f1_score, fbeta_score, precision_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, recall_score, average_precision_score, log_loss\n",
    "# from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "\n",
    "from flask import Flask, jsonify\n",
    "from flask_cors import CORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648cd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda update --all\n",
    "# conda install -c conda-forge xgboost\n",
    "# conda install graphviz python-graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d902bd5",
   "metadata": {},
   "source": [
    "# Connect Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a88a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_redshift(credentials):  \n",
    "    \n",
    "    # connect redshift\n",
    "    connection = psycopg2.connect(\n",
    "        host=credentials['host'],\n",
    "        port=credentials['port'],\n",
    "        dbname=credentials['dbname'],\n",
    "        user=credentials['user'],\n",
    "        password=credentials['password'])\n",
    "    \n",
    "    # initialize cursor objects\n",
    "    cur = connection.cursor()\n",
    "    \n",
    "    return cur   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d211ff0",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00759444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cur, query):    \n",
    "    \n",
    "    # load data\n",
    "    cur.execute(query)\n",
    "    \n",
    "    # frame the data\n",
    "    df = pd.DataFrame(cur.fetchall())\n",
    "    df.columns = [desc[0] for desc in cur.description]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ea2fb",
   "metadata": {},
   "source": [
    "# Pre-Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab9465",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8406daa",
   "metadata": {},
   "source": [
    "#### 1.0 Outliers Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5321072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function which remove samples, where a given feature is above/below a given threshold.\n",
    "\n",
    "def remove_outliers(df, feature:str, threshold:int, above_or_below:str):    \n",
    "    \n",
    "    # find index of outliers\n",
    "    if above_or_below =='above':\n",
    "        drop_index = set(df[df[feature]>threshold].index) \n",
    "    elif above_or_below =='below':\n",
    "        drop_index = set(df[df[feature]<threshold].index)\n",
    "    else:\n",
    "        raise ValueError('Wrong above_or_below input')\n",
    "    \n",
    "    # get index of df without outliers\n",
    "    new_index = list(set(df.index) - set(drop_index))\n",
    "    \n",
    "    # remove outliers\n",
    "    df = df.iloc[new_index]\n",
    "    \n",
    "    # reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482be8bf",
   "metadata": {},
   "source": [
    "#### 1.1 Bucketing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fccb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top X most common values of a feature\n",
    "\n",
    "def feature_top_x(df, feature, top_x=None):\n",
    "    data = df[feature].value_counts(normalize=True).head(top_x).to_frame().reset_index()\n",
    "    top_x_data = list(data.iloc[:,0])\n",
    "    return top_x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458d2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X buckets of a feature\n",
    "\n",
    "def feature_bucket_based_on_top_x(row, feature, top_x_list):      \n",
    "    if pd.isnull(row[feature]):\n",
    "        return None\n",
    "    elif(row[feature] in top_x_list):\n",
    "        return row[feature]\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9a8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the relevant bucket for each sample in the data set \n",
    "\n",
    "def calculate_feature_bucked(df, feature, top_x):\n",
    "    top_x_list = feature_top_x(df, feature, top_x)\n",
    "    df[f\"{feature}_bucket\"] = df.apply(lambda row: feature_bucket_based_on_top_x(row, feature, top_x_list) ,axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028a5ee",
   "metadata": {},
   "source": [
    "#### 1.2 Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c9ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(df, outliers_to_be_removed):         \n",
    "    \n",
    "    # remove outliers\n",
    "    for feature, values in outliers_to_be_removed.items():\n",
    "        df = remove_outliers(df, feature, values['threshold'], values['above_or_below'])\n",
    "    \n",
    "    # create buckets of categorical features\n",
    "    features_to_bucket = {'most_played_game':12,\n",
    "                          'country':10,\n",
    "                          'language':10\n",
    "                         }\n",
    "    # convert categorical features to buckets\n",
    "    for k, v in features_to_bucket.items():\n",
    "        df =  calculate_feature_bucked(df,k,v) \n",
    "\n",
    "    # drop columns\n",
    "    col_to_drop = ['channel_id', 'application_id', \n",
    "                   'country', 'language', 'most_played_game']\n",
    "    df = df.drop(col_to_drop, axis=1)\n",
    "\n",
    "    # convert numeric features to category\n",
    "    numeric_to_category = ['is_tipping_panel', 'is_bot_command_usage',\n",
    "                           'is_overlay', 'is_website_visit',\n",
    "                           'is_se_live', 'is_alert_box_fired',\n",
    "                           'is_open_stream_report']\n",
    "    for feature in numeric_to_category:\n",
    "        df[feature] = df[feature].astype(object)\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac302f",
   "metadata": {},
   "source": [
    "#### 1.3 Data Preparation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea898cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df):    \n",
    "    \n",
    "    # Declare feature vector and target variable\n",
    "    X = df.drop('is_accept', axis=1)\n",
    "    y = df['is_accept']\n",
    "    \n",
    "    # Create dummy variables\n",
    "    X = pd.get_dummies(X)\n",
    "    \n",
    "    # split the dataset\n",
    "    return train_test_split(X, y, test_size=0.3, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004feb76",
   "metadata": {},
   "source": [
    "### 2. Models Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e320837",
   "metadata": {},
   "source": [
    "#### 2.1 Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e56241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(model, X_test, y_test):\n",
    "    res = {}\n",
    "    \n",
    "    # predict with the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # model scoring\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    logistic_loss = log_loss(y_test, y_pred)\n",
    "    \n",
    "    # print scores\n",
    "    res = {'RMSE': rmse, 'R2': r2, 'Log Loss': logistic_loss}\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7573370",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1dc3b8",
   "metadata": {},
   "source": [
    "### 1. Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720778de",
   "metadata": {},
   "source": [
    "#### 1.1 Optuna Optimization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86bf86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable printing\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# create objective function which returns scoring (numerical value to evaluate the performance of the hyperparameters)\n",
    "def objective(trial, eval_model, param, score_func, X_train, y_train, X_test, y_test):    \n",
    "    \n",
    "    # hyperparameters to be tuned\n",
    "    hyperparameters_candidates = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.05),\n",
    "        \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.2, 0.6),\n",
    "        \"subsample\": trial.suggest_loguniform(\"subsample\", 0.4, 0.8),\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 10.0),\n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n",
    "        \"gamma\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n",
    "        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 10, 1000),\n",
    "        \"scale_pos_weight\": trial.suggest_int('scale_pos_weight', 1, 100)\n",
    "    }\n",
    "    \n",
    "    # instantiate the model \n",
    "    xgb_optuna = eval_model(**param, **hyperparameters_candidates)\n",
    "    \n",
    "    # Add a callback for pruning (ensure unpromising trials are stopped early)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-aucpr\")\n",
    "    \n",
    "    # fit\n",
    "    xgb_optuna.fit(X_train,\n",
    "                   y_train,\n",
    "                   verbose=False,\n",
    "                   eval_set=[(X_test, y_test)],\n",
    "                   callbacks=[pruning_callback]) \n",
    "    \n",
    "    # tag each trial with keyword\n",
    "    trial.set_user_attr(key=\"model\", value=xgb_optuna)\n",
    "    \n",
    "    # predict with the model\n",
    "    y_pred = xgb_optuna.predict(X_test)\n",
    "\n",
    "\n",
    "    # score the model\n",
    "    score = score_func(y_pred)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3a18f",
   "metadata": {},
   "source": [
    "#### 1.2 Retrieving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e232d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a callback function to retrive the best model (i.e. the model with the best hyperparameters)\n",
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key=\"best_model\", value=trial.user_attrs[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82340e",
   "metadata": {},
   "source": [
    "### 2. Loss Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a260a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePrecisionScore:\n",
    "    def __init__(self, y_test, direction):\n",
    "        self.y_test = y_test\n",
    "        self.direction = direction\n",
    "        self.name = \"average_precision_score\"\n",
    "    def score(self, y_pred):\n",
    "        return average_precision_score(self.y_test, y_pred)\n",
    "\n",
    "    \n",
    "class LogLoss:\n",
    "    def __init__(self, y_test, direction):\n",
    "        self.y_test = y_test\n",
    "        self.direction = direction\n",
    "        self.name = \"log_loss\"\n",
    "    def score(self, y_pred):\n",
    "        return log_loss(self.y_test, y_pred)\n",
    "\n",
    "\n",
    "    \n",
    "class RMSE:\n",
    "    def __init__(self, y_test, squared, direction):\n",
    "        self.y_test = y_test\n",
    "        self.squared = False\n",
    "        self.direction = direction\n",
    "        self.name = \"mean_squared_error\"\n",
    "    def score(self, y_pred):\n",
    "        return mean_squared_error(self.y_test, y_pred, squared=self.squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a5130",
   "metadata": {},
   "source": [
    "### 3. Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfa501",
   "metadata": {},
   "source": [
    "#### 3.1 Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b36aeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_models_hyperparams(eval_model, param, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # dictionary for saving models\n",
    "    grids = {}\n",
    "\n",
    "\n",
    "    # define scoring method\n",
    "    scoring_objects = [AveragePrecisionScore(y_test, direction=\"maximize\"),\n",
    "                       LogLoss(y_test, direction=\"minimize\"),\n",
    "                       RMSE(y_test, squared=False, direction=\"minimize\")\n",
    "                      ]\n",
    "\n",
    "\n",
    "    # create a sampler object to find more efficiently the best hyperparameters\n",
    "    sampler = TPESampler()  # by default the sampler = TPESampler()\n",
    "\n",
    "\n",
    "    for score_obj in scoring_objects:\n",
    "\n",
    "        # create a study object to set the direction of optimization and the sampler\n",
    "        study = optuna.create_study(sampler=sampler, direction = score_obj.direction)\n",
    "\n",
    "        # run the study object\n",
    "        study.optimize(lambda trial: objective(trial,\n",
    "                                               eval_model,\n",
    "                                               param,\n",
    "                                               score_obj.score,\n",
    "                                               X_train,\n",
    "                                               y_train,\n",
    "                                               X_test,\n",
    "                                               y_test), # make smart guesses where the best values hyperparameters\n",
    "                       n_trials = 1, # try hyperparameters combinations n_trials times\n",
    "                       callbacks=[callback]) # callback save the best model\n",
    "\n",
    "        # name the best model\n",
    "        model_name = \"xgb_reg_optuna_\" + score_obj.name\n",
    "        grids[model_name] = {}\n",
    "\n",
    "        # initiate best model\n",
    "        model_object = study.user_attrs[\"best_model\"]\n",
    "        grids[model_name][\"model_object\"] = model_object\n",
    "\n",
    "        # score best model\n",
    "        model_scores = model_performance(model_object, X_test, y_test)\n",
    "        grids[model_name][\"model_scores\"] = model_scores\n",
    "    \n",
    "    return grids\n",
    "\n",
    "#     # open a file where we store the model\n",
    "#     model_file = open(f'./{model_name}.pkl', 'wb')\n",
    "#     grids[model_name][\"model_file\"] = model_file\n",
    "                      \n",
    "#     # dump model to that  file\n",
    "#     pickle.dump(model_object, model_file) \n",
    "    \n",
    "#     # close the file\n",
    "#     model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "274334c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid_results(grids):   \n",
    "    \n",
    "    # print models results\n",
    "    for name, model in grids.items():\n",
    "        print('{:-^70}'.format(' [' + name + '] '))\n",
    "        print(model[\"model_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa5e5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(grids):     \n",
    "    \n",
    "    # filter models with negative R^2 (models worse than a constant function that always predicts the mean of the data)\n",
    "    new_grids = {k: v for k, v in grids.items() if v[\"model_scores\"]['R2']>0}\n",
    "    \n",
    "    # retrieve best model\n",
    "    if bool(new_grids):\n",
    "        best_model_item = min(new_grids.items(), key=lambda x: x[1][\"model_scores\"]['Log Loss'])\n",
    "        best_model_object = best_model_item[1][\"model_object\"]\n",
    "    else: \n",
    "        raise ValueError(\"\\033[1m\" + 'No model has fitted the data well.' + \"\\033[0m\")\n",
    "            \n",
    "    return best_model_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b24b2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(best_model_object):    \n",
    "    \n",
    "    # open (and close) a file where we store the best model\n",
    "    with open('model.pkl', 'wb') as f:\n",
    "        \n",
    "        # dump best model to the file \n",
    "        pickle.dump(best_model_object, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95950ccc",
   "metadata": {},
   "source": [
    "#### 3.2 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1e3f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {'host':\"172.31.0.207\",\n",
    "               'port':5439,\n",
    "               'dbname':\"streamelements\",\n",
    "               'user':\"yehuda\",\n",
    "               'password':'SEdontdaytrade1337ageofempires'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee5efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select channel_id\n",
    "     , application_id\n",
    "     , se_age_days::float\n",
    "     , country\n",
    "     , language\n",
    "     , ccv_30_d::float\n",
    "     , ccv_60_d::float\n",
    "     , ccv_growth_60_30_d::float\n",
    "     , most_played_game\n",
    "     , cnt_streams::float\n",
    "     , invitations_l3m::float\n",
    "     , acceptances_l3m::float\n",
    "     , deployments_l3m::float\n",
    "     , rejections_l3m::float\n",
    "     , offer_page_visits_l3m::float\n",
    "     , invitations_l6m::float\n",
    "     , acceptances_l6m::float\n",
    "     , deployments_l6m::float\n",
    "     , rejections_l6m::float\n",
    "     , offer_page_visits_l6m::float\n",
    "     , hours_streamed::float\n",
    "     , hours_watched::float\n",
    "     , total_chatters::float\n",
    "     , is_tipping_panel\n",
    "     , is_bot_command_usage\n",
    "     , is_overlay\n",
    "     , is_website_visit\n",
    "     , is_se_live\n",
    "     , is_alert_box_fired\n",
    "     , cnt_alert_box_fired::float\n",
    "     , is_open_stream_report\n",
    "     , campaigns_revenue::float\n",
    "     , tips::float\n",
    "     , tips_revenue::float\n",
    "     , on_screen_cheers::float\n",
    "     , on_screen_cheers_revenue::float\n",
    "     , on_screen_subs::float\n",
    "     , on_screen_subs_revenue::float\n",
    "     , is_accept\n",
    "     , campaign_game\n",
    "--     , campaign_max_payout::float\n",
    "--     , campaign_max_payout_per_ccv::float\n",
    "from bi_dev.acceptance_rate_model_training\n",
    "where campaign_category = 'games' and\n",
    "      se_age_days>0 and\n",
    "      invite_timestamp >= '2022-01-01' \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "681e4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers to be removed\n",
    "outliers_to_be_removed = {'ccv_30_d':{'threshold':700,'above_or_below':'above'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "044393e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "param = {\"objective\": \"binary:logistic\", # logistic regression for binary classification, output probability\n",
    "         \"missing\": np.nan, # whenever a null values is encountered it is treated as missing value\n",
    "         \"seed\": 42, # used to generate the folds\n",
    "         \"tree_method\": \"gpu_hist\", # speed up processing by using gpu power\n",
    "         \"early_stopping_rounds\": 50, # overfitting prevention, stop early if no improvement in learning\n",
    "         \"eval_metric\": \"aucpr\", # evaluation metric for validation data\n",
    "         \"n_estimators\": 10000 # number of trees\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8537d17",
   "metadata": {},
   "source": [
    "### 4. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b14dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(credentials, query, param, outliers_to_be_removed):     \n",
    "    \n",
    "    # connect redshift\n",
    "    cur = connect_redshift(credentials)   \n",
    "    \n",
    "    # load data from redshift\n",
    "    df = load_data(cur, query)\n",
    "        \n",
    "    # data processing \n",
    "    df = data_processing(df, outliers_to_be_removed) \n",
    "    \n",
    "    # data split\n",
    "    X_train, X_test, y_train, y_test = data_split(df) \n",
    "    \n",
    "    # define model to pass\n",
    "    eval_model = xgb.XGBRegressor \n",
    "    \n",
    "    # save models in a dictionary \n",
    "    grids = tune_models_hyperparams(eval_model, param, X_train, y_train, X_test, y_test) \n",
    "    \n",
    "    # print models\n",
    "    print_grid_results(grids) \n",
    "    \n",
    "    # extract best model\n",
    "    best_model_object = get_best_model(grids) \n",
    "    \n",
    "    # save best model in a pickle file\n",
    "    save_best_model(best_model_object) \n",
    "    \n",
    "    return best_model_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a8495fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main(credentials, query, param, outliers_to_be_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b286eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_training.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabi_gpu",
   "language": "python",
   "name": "gabi_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
